{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import regex as re  # https://pypi.org/project/regex/\n",
    "import stanza\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import display  # noqa: A004\n",
    "from ipywidgets import IntProgress\n",
    "from nltk import RegexpParser, Tree\n",
    "from pyjarowinkler.distance import get_jaro_distance\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'none'\n",
    "%alias open_html open\n",
    "plt.style.use(['dark_background'])\n",
    "\n",
    "# If necessary, uncomment these lines to add language data,\n",
    "# stanza.download('la', package='ittb')\n",
    "# stanza.download('la', package='proiel')\n",
    "# stanza.download('la', package='perseus')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ancillary functions\n",
    "highlights = 0  # track highlights\n",
    "parse_complete = 0  # track fully parsed sentences\n",
    "\n",
    "\n",
    "def p_bar(mx, desc):\n",
    "    \"\"\"Set up a progress bar.\"\"\"\n",
    "    global p  # noqa: PLW0603\n",
    "    p = IntProgress(min=0, max=mx, description=desc)\n",
    "    return p\n",
    "\n",
    "\n",
    "def is_pn(token, threshold=0.8):\n",
    "    \"\"\"Return True if the token is a personal name, False otherwise.\n",
    "\n",
    "    Takes a token and compares it to a list of personal names.\n",
    "    Returns True if the Jaro-Winkler distance is above threshold,\n",
    "    otherwise returns False.\n",
    "    \"\"\"\n",
    "    token = token.strip().lower()\n",
    "    return any(\n",
    "        get_jaro_distance(\n",
    "            token,\n",
    "            pn,\n",
    "            winkler=True,\n",
    "            scaling=0.1,\n",
    "        )\n",
    "        > threshold\n",
    "        for pn in proper_nouns\n",
    "    )\n",
    "\n",
    "\n",
    "def element_to_html(element):\n",
    "    \"\"\"Convert NLTK element (token or tree) to HTML representation.\"\"\"\n",
    "    _html = ''\n",
    "    if type(element) is Tree:\n",
    "        tag = element.label().split('.')[0] if '.' in element.label() else element.label()\n",
    "        colour = colours.get(tag.split('-')[0] if '-' in tag else tag)\n",
    "\n",
    "        highlight = ''\n",
    "        if 'HI' in element.label():\n",
    "            highlight = 'hi'\n",
    "            global highlights  # noqa: PLW0603\n",
    "            highlights += 1\n",
    "\n",
    "        _html = f'<div class=\"unit\"> \\\n",
    "            <div class=\"unit_label\" style=\"color: {colour};\" title=\"{key.get(tag)}\"> \\\n",
    "            {tag}</div><div class=\"unit_content {highlight}\" style=\"border-color: {colour};\">'\n",
    "        for subelement in element:\n",
    "            if type(subelement) is Tree:\n",
    "                _html += element_to_html(subelement)\n",
    "            else:\n",
    "                # if subelement[0] != '.' and 'PUNCT' not in subelement[1]:\n",
    "                _html += f'<div class=\"token\" title=\"{subelement[1]}\">{subelement[0]}</div>'\n",
    "\n",
    "        _html += '</div></div>'\n",
    "\n",
    "    else:\n",
    "        # if element[0] != '.':\n",
    "        _html += f'<div class=\"token\" title=\"{element[1]}\">{element[0]}</div>'\n",
    "\n",
    "    return _html\n",
    "\n",
    "\n",
    "def render_dataset(tree, start=0, end=100):\n",
    "    \"\"\"Generate an annotated HTML representation from an NLTK tree.\n",
    "\n",
    "    The start and end parameters determine how much of the tree is processed.\n",
    "    \"\"\"\n",
    "    global highlights  # noqa: PLW0603\n",
    "    highlights = 0\n",
    "    status = ''\n",
    "\n",
    "    result = '<!DOCTYPE html><html><head><meta charset=\"utf-8\">\\\n",
    "                <link rel=\"stylesheet\" href=\"viz_styles.css\"/></head><body>'\n",
    "\n",
    "    for i, sentence in enumerate(tree[start:], start=1):\n",
    "        if [e for e in sentence if type(e) is Tree]:\n",
    "            status = 'red'\n",
    "            if all(type(e) is Tree for e in sentence):\n",
    "                status = 'green'\n",
    "\n",
    "        result += f'<div class=\"sentence\"><div class=\"line_no {status}\">{i}</div><div class=\"container\">'\n",
    "        for el in sentence:\n",
    "            result += element_to_html(el)\n",
    "\n",
    "        if end and i == end:\n",
    "            break\n",
    "\n",
    "        result += '</div></div>'\n",
    "\n",
    "    result += '</body></html>'\n",
    "\n",
    "    if highlights:\n",
    "        print(f'{highlights} highlighted elements.')\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def compile_statistics(tree):\n",
    "    \"\"\"Compile statistics from an NLTK tree.\"\"\"\n",
    "    parsed_sentences = 0\n",
    "    ops = 0\n",
    "\n",
    "    def parse(element, ops):\n",
    "        if type(element) is Tree:\n",
    "            if 'OP' in element.label():\n",
    "                ops += 1\n",
    "\n",
    "            for subelement in element:\n",
    "                if type(subelement) is Tree:\n",
    "                    ops = parse(subelement, ops)\n",
    "        return ops\n",
    "\n",
    "    for sentence in tree:\n",
    "        if all(type(e) is Tree for e in sentence):\n",
    "            parsed_sentences += 1\n",
    "\n",
    "        for element in sentence:\n",
    "            ops = parse(element, ops)\n",
    "\n",
    "    return {'parsed': parsed_sentences, 'ops': ops}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean dataset\n",
    "with open('data/lines_clean_dev.txt') as file:  # dev lines\n",
    "    corpus = file.read()\n",
    "\n",
    "# load glosses\n",
    "with open('data/gloss_dictionary.json') as file:\n",
    "    lemma_dict = json.loads(file.read())\n",
    "\n",
    "# load noun list\n",
    "with open('data/nouns.json') as file:\n",
    "    nouns = json.loads(file.read())\n",
    "\n",
    "# load proper noun list\n",
    "with open('data/proper_nouns.json') as file:\n",
    "    proper_nouns = [pn.lower() for pn in json.loads(file.read())]\n",
    "\n",
    "\n",
    "# load unit list\n",
    "with open('data/measurement_units.json') as file:\n",
    "    units = json.loads(file.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special words and lemmata for which the lemma should be included in the POS tags\n",
    "special_words = [\n",
    "    'item',\n",
    "    'et',\n",
    "    'cum',\n",
    "    'in',\n",
    "    'de',\n",
    "    'primo',\n",
    "    'ad',\n",
    "    'sive',\n",
    "    'seu',\n",
    "    'an',\n",
    "    'lo',\n",
    "    'aliquos',\n",
    "]\n",
    "\n",
    "special_lemmata = [\n",
    "    'domina',\n",
    "    'dominus',\n",
    "    'sanctus',\n",
    "    'dictus',\n",
    "    'dico',\n",
    "    'alius',\n",
    "    'subdo',\n",
    "    'uxor',\n",
    "    'dimidium',\n",
    "    'circa',\n",
    "    'sine',\n",
    "    'medius',\n",
    "    'idem',\n",
    "    'supradico',\n",
    "    'habeo',\n",
    "    'mulier',\n",
    "    'homo',\n",
    "    'muliebris',\n",
    "    'situs',\n",
    "    'scio',\n",
    "    'quidam',\n",
    "    'sistere',\n",
    "    'pro',\n",
    "    'pignus',\n",
    "    'folrare',\n",
    "    'camera',\n",
    "    'appello',\n",
    "    'vocare',\n",
    "    'iuxta',\n",
    "    'isque',\n",
    "    'aliqui',\n",
    "    'sanus',\n",
    "    'sequor',\n",
    "    'clausa',\n",
    "    'garnire',\n",
    "    'frezatus',\n",
    "    'munire',\n",
    "    'sum',\n",
    "    'scilicet',\n",
    "    'modus',\n",
    "    'alias',\n",
    "    'qui',\n",
    "    'praedictus',\n",
    "    'ponere',\n",
    "    'atque',\n",
    "    'meianus',\n",
    "    'confrontare',\n",
    "    'apud',\n",
    "    'folratura',\n",
    "    'impignorare',\n",
    "    'invenio',\n",
    "    'hic',\n",
    "    'bonum',\n",
    "    'prout',\n",
    "    'infra',\n",
    "    'ecce',\n",
    "    'parafernalia',\n",
    "    'dos',\n",
    "    'jocalis',\n",
    "    'reperire',\n",
    "    'mobilis',\n",
    "    'peto',\n",
    "    'condam',\n",
    "    'omnis',\n",
    "    'annus',\n",
    "    'servire',\n",
    "    'moneta',\n",
    "    'currere',\n",
    "    'valeo',\n",
    "    'ascendere',\n",
    "    'ultra',\n",
    "    'avi',\n",
    "    'maternus',\n",
    "    'ipse',\n",
    "    'frater',\n",
    "    'frodium',\n",
    "    'dicere',\n",
    "    'videlicet',\n",
    "    'terra',\n",
    "    'depingere',\n",
    "    'mutuare',\n",
    "    'tenere',\n",
    "    'causa',\n",
    "    'mutuum',\n",
    "    'manus',\n",
    "    'per',\n",
    "    'armigeratus',\n",
    "    'munio',\n",
    "    'oratio',\n",
    "    'describere',\n",
    "]\n",
    "\n",
    "# add units extracted manually to list\n",
    "units = [\n",
    "    *units,\n",
    "    'solidus',\n",
    "    'florenus',\n",
    "    'denarius',\n",
    "    'currus',\n",
    "    'grossus',\n",
    "    'scutum',\n",
    "    'censualis',\n",
    "]\n",
    "\n",
    "numerals = [\n",
    "    'unus',\n",
    "    'duo',\n",
    "    'duodena',\n",
    "    'duodenus',\n",
    "    'sex',\n",
    "]\n",
    "\n",
    "locations = {\n",
    "    'camera': 'room',\n",
    "    'aula': 'room',\n",
    "    'coquina': 'room',\n",
    "    'cellarium': 'room',\n",
    "    'transversia': 'lm',\n",
    "    'hospicium': 'imm',\n",
    "    'socolus': 'room',\n",
    "    'capella': 'room',\n",
    "    'bastida': 'lm',\n",
    "    'domus': 'lm',\n",
    "    'molendinum': 'imm',\n",
    "    'carreria': 'lm',\n",
    "    'portus': 'lm',\n",
    "    'porticus': 'room',\n",
    "    'operatorium': 'imm',\n",
    "    'lignum': 'imm',\n",
    "    'viridarium': 'imm',\n",
    "    'canton': 'lm',\n",
    "    'canto': 'lm',\n",
    "    'monialis': 'lm',\n",
    "    'vinea': 'imm',\n",
    "    'macellarius': 'imm',\n",
    "    'saux': 'lm',\n",
    "}\n",
    "\n",
    "typos = {\n",
    "    'itum': ('item', 'item'),\n",
    "    'quoddum': ('quoddam', 'quidam'),\n",
    "    'bassinteum': ('bassinetum', 'bacinus'),\n",
    "}\n",
    "\n",
    "# define key and colour concordance (used to visualise results)\n",
    "key = {\n",
    "    'AGT': 'Agent',\n",
    "    'AP-Cu': 'Asset phrase - currency',\n",
    "    'AP-Loan': 'Asset phrase - loan related',\n",
    "    'AP-Oth': 'Asset phrase - other',\n",
    "    'AP-Rs': 'Asset phrase - real state',\n",
    "    'ATT': 'Attribute',\n",
    "    'ATT-Abl': 'Attribute - ablative',\n",
    "    'ATT-Add': 'Attribute - additive',\n",
    "    'ATT-Adj': 'Attribute - adjectival',\n",
    "    'ATT-Cont': 'Attribute - containment',\n",
    "    'ATT-Deco': 'Attribute - decorative',\n",
    "    'ATT-Gen': 'Attribute - genitive',\n",
    "    'ATT-Gend': 'Attribute - gender',\n",
    "    'ATT-Ger': 'Attribute - gerundive',\n",
    "    'ATT-Mag': 'Attribute - magnitude',\n",
    "    'ATT-Part': 'Attribute - participial',\n",
    "    'ATT-Pos': 'Attribute - posession',\n",
    "    'ATT-Ref': 'Attribute - reference',\n",
    "    'ATT-Stat': 'Attribute - state',\n",
    "    'ATT-Sub': 'Attribute - substractive',\n",
    "    'LOC-lm': 'Location - landmark',\n",
    "    'LOC-rm': 'Location - room',\n",
    "    'LOC-rs': 'Location - immobilia',\n",
    "    'NAME': 'Name',\n",
    "    'OBJ': 'Object',\n",
    "    'OBJ-Alt': 'Object - alternates',\n",
    "    'OBJ-c': 'Object - compound',\n",
    "    'OBJ-x': 'Object - estimated/contextual',\n",
    "    'OP': 'Object phrase',\n",
    "    'OP-Cont': 'Object phrase - container',\n",
    "    'pAbl': 'Particle - ablative',\n",
    "    'pAdd': 'Particle - additive',\n",
    "    'pAlt': 'Particle - alternative',\n",
    "    'pComp': 'Particle - comparative',\n",
    "    'pCont': 'Particle - containement',\n",
    "    'pCoor': 'Particle - coordinating conjunction',\n",
    "    'pCtx': 'Particle - context',\n",
    "    'pDeco': 'Particle - decorative',\n",
    "    'pList': 'Particle - list marker',\n",
    "    'pLoc': 'Particle - locative',\n",
    "    'pPurp': 'Particle - purpose/function',\n",
    "    'pQual': 'Particle - qualification',\n",
    "    'pRef': 'Particle - reference',\n",
    "    'pRnd': 'Particle - rounding',\n",
    "    'pStat': 'Particle - state',\n",
    "    'pSub': 'Particle - substrative',\n",
    "    'QT': 'Quantity',\n",
    "    'QT-Gen': 'Quantity - genitive',\n",
    "    'RP': 'Room phrase',\n",
    "    'UNIT': 'Unit of measurement',\n",
    "}\n",
    "\n",
    "colours = {\n",
    "    '1': '#ffffff',  # white\n",
    "    'NAME': '#ead03a',  # yellow\n",
    "    'LOC': '#a27230',  # brown\n",
    "    '4': '#ea8f3a',  # orange\n",
    "    'AP': '#ff6e6e',  # red\n",
    "    'UNIT': '#ffa7b5',  # pink\n",
    "    'RP': '#db5ce0',  # magenta\n",
    "    'AGT': '#8c5ce0',  # purple\n",
    "    'QT': '#006cff',  # blue\n",
    "    'NP': '#3a9eea',  # light blue\n",
    "    'OP': '#3aea74',  # light green\n",
    "    'ALT': '#30a250',  # green\n",
    "    'OBJ': '#b2d29f',  # mint green\n",
    "    'ATT': '#2ebdad',  # cyan\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 16:21:18 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b613545c88f46af8197be6ecc1cbf8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 16:21:18 INFO: Downloaded file to /Users/gabep/stanza_resources/resources.json\n",
      "2025-06-02 16:21:18 INFO: Loading these models for language: la (Latin):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | ittb          |\n",
      "| pos       | ittb_nocharlm |\n",
      "| lemma     | ittb_nocharlm |\n",
      "=============================\n",
      "\n",
      "2025-06-02 16:21:18 INFO: Using device: cpu\n",
      "2025-06-02 16:21:18 INFO: Loading: tokenize\n",
      "2025-06-02 16:21:18 INFO: Loading: pos\n",
      "2025-06-02 16:21:19 INFO: Loading: lemma\n",
      "2025-06-02 16:21:19 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus analysis completed.\n"
     ]
    }
   ],
   "source": [
    "# create a configuration object for Stanza\n",
    "config = {\n",
    "    'processors': 'tokenize,pos,lemma',\n",
    "    'lang': 'la',\n",
    "    'tokenize_pretokenized': True,\n",
    "}\n",
    "\n",
    "# create a Stanza pipeline and analyse the corpus\n",
    "nlp = stanza.Pipeline(**config)\n",
    "doc = nlp(corpus)\n",
    "print('Corpus analysis completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4bee1872d44eb4ad6077843ce3fc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Compiling tree', max=204)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 204 sentences...\n",
      "Extraction completed on 204 sentences.\n"
     ]
    }
   ],
   "source": [
    "# extract tokens with POS-tags and apply overrides\n",
    "tagged_sentences = []  # store results\n",
    "lemmata = {}  # also generate a word -> lemma concordance\n",
    "total_sentences = len(doc.sentences)\n",
    "display(p_bar(total_sentences, 'Compiling tree'))  # show progress bar\n",
    "print(f'Processing {total_sentences} sentences...')\n",
    "\n",
    "for sentence in doc.sentences:\n",
    "    sent = []\n",
    "    for word in sentence.words:\n",
    "        _pos = word.pos\n",
    "        _xpos = word.xpos\n",
    "        _feats = word.feats if word.feats else 'Feats=0'\n",
    "        _lemma = word.lemma\n",
    "        _word = word.text\n",
    "\n",
    "        if typos.get(word.text):\n",
    "            _word = typos[word.text][0]\n",
    "            _lemma = typos[word.text][1]\n",
    "\n",
    "        # apply overrides\n",
    "        if _lemma in nouns:\n",
    "            _feats += '|Object=1'\n",
    "            if _pos == 'PUNCT':\n",
    "                _pos = 'NOUN'\n",
    "\n",
    "        if _pos == 'PUNCT':\n",
    "            _pos = 'ADJ'\n",
    "            _feats = 'Feats=0'\n",
    "\n",
    "        if _lemma in numerals or _word.isdigit() or re.fullmatch(r'[ivxcm]+', _word):\n",
    "            _pos = 'NUM'\n",
    "\n",
    "        if _lemma in units:\n",
    "            _feats += '|Function=Unit'\n",
    "            # _pos = 'UNIT'\n",
    "\n",
    "        if _word in proper_nouns:\n",
    "            # if is_pn(word.text.lower(), 0.95):\n",
    "            # _pos = 'PNAME'\n",
    "            _feats += '|Capital=1'\n",
    "\n",
    "        if _lemma in locations:\n",
    "            _feats += f'|Function=Location|Type={locations[_lemma]}'\n",
    "            # _pos = 'LOC'\n",
    "\n",
    "        # add lemma for special cases\n",
    "        if _word in special_words or _lemma in special_lemmata:\n",
    "            token = (_word, f'{_pos}-{_feats}|Lemma={_lemma}')\n",
    "        else:\n",
    "            token = (_word, f'{_pos}-{_feats}')\n",
    "\n",
    "        sent.append(token)\n",
    "        lemmata[_word] = _lemma\n",
    "\n",
    "    tagged_sentences.append(sent)\n",
    "    p.value += 1\n",
    "\n",
    "print(f'Extraction completed on {len(tagged_sentences)} sentences.')\n",
    "\n",
    "# persist the lemmata to disk\n",
    "with open('results/lemmata.pickle', 'wb') as file:\n",
    "    pickle.dump(lemmata, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0520ae752b8d45c8b035c997d7867bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Chunking corpus', max=204)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking completed. 204 sentences processed.\n"
     ]
    }
   ],
   "source": [
    "# open file with chunking rules\n",
    "with open('rules/chunk_rules.chk') as file:\n",
    "    rules = file.readlines()\n",
    "\n",
    "# parse and generate grammar\n",
    "rg_grammar = ''\n",
    "\n",
    "for rule in rules:\n",
    "    if rule.strip() and rule[0] != '#':\n",
    "        clean_rule = f'{rule[: rule.index(\"#\")]}\\n' if '#' in rule else rule\n",
    "        rg_grammar += clean_rule\n",
    "\n",
    "# chunk the dataset based on the rules we defined\n",
    "rg_parser = RegexpParser(rg_grammar)  # define a chunker with our grammar\n",
    "parsed_sentences = []  # define list to store parsed data\n",
    "display(p_bar(total_sentences, 'Chunking corpus'))  # show progress bar\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    parsed_sentences.append(rg_parser.parse(sentence))\n",
    "    p.value += 1\n",
    "\n",
    "print(f'Chunking completed. {len(parsed_sentences)} sentences processed.')\n",
    "\n",
    "# persist the parsed sentences\n",
    "with open('results/parsed_sentences.pickle', 'wb') as file:\n",
    "    pickle.dump(parsed_sentences, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 out of 204 sentences fully parsed (1.96%)\n",
      "Previous run: 4 sentences (1.96%)\n"
     ]
    }
   ],
   "source": [
    "# visualize the results\n",
    "viz_data = render_dataset(parsed_sentences, 0, 0)\n",
    "\n",
    "stats = compile_statistics(parsed_sentences)\n",
    "print(\n",
    "    f'{stats[\"parsed\"]} out of {total_sentences} sentences fully parsed',\n",
    "    f'({round(stats[\"parsed\"] * 100 / total_sentences, 2)}%)',\n",
    ")\n",
    "\n",
    "if parse_complete:\n",
    "    print(f'Previous run: {parse_complete} sentences ({round(parse_complete * 100 / total_sentences, 2)}%)')\n",
    "\n",
    "parse_complete = stats['parsed']\n",
    "\n",
    "# save results to a local file\n",
    "with open('results-viz/chunks.html', 'w') as file:\n",
    "    file.write(viz_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# open visualization in new tab\n",
    "!open 'results-viz/chunks.html'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204 shallow sentences generated.\n"
     ]
    }
   ],
   "source": [
    "# clean the tree to keep only level 1 tags\n",
    "l1_tags = {\n",
    "    'OP': 'OBJECT',\n",
    "    'OP-Cont': 'CONTAINER',\n",
    "    'RP': 'ROOM',\n",
    "    'AP-Cu': 'CURRENCY',\n",
    "    'AP-Rs': 'PROPERTY',\n",
    "    'AP-Loan': 'LOAN',\n",
    "    'AP-Oth': 'ASSET',\n",
    "}\n",
    "\n",
    "shallow_tree_sentences = []\n",
    "\n",
    "for sentence in parsed_sentences:\n",
    "    clean_sentence = []\n",
    "    for element in sentence:\n",
    "        if type(element) is Tree:\n",
    "            if element.label() in l1_tags:\n",
    "                clean_sentence.append(Tree(l1_tags[element.label()], element.flatten().leaves()))\n",
    "            else:\n",
    "                clean_sentence += element.flatten().leaves()\n",
    "        else:\n",
    "            clean_sentence.append(element)\n",
    "\n",
    "    shallow_tree_sentences.append(Tree('S', clean_sentence))\n",
    "\n",
    "print(f'{len(shallow_tree_sentences)} shallow sentences generated.')\n",
    "# shallow_tree_sentences[0].draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist the parsed sentences\n",
    "with open('results/parsed_sentences_shallow.pickle', 'wb') as file:\n",
    "    pickle.dump(shallow_tree_sentences, file)\n",
    "\n",
    "# visualize the results\n",
    "viz_shallow_data = render_dataset(shallow_tree_sentences, 0, 0)\n",
    "\n",
    "# save results to a local file\n",
    "with open('results-viz/chunks_shallow.html', 'w') as file:\n",
    "    file.write(viz_shallow_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open visualization in new tab\n",
    "!open 'results-viz/chunks_shallow.html'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
